{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucanyckees/Desktop/my-repos/recommendation_system/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple dataset with triplets (anchor, positive, negative text)\n",
    "class TripletTextDataset(Dataset):\n",
    "    def __init__(self, anchor_texts, positive_texts, negative_texts, tokenizer, max_len=32):\n",
    "        self.anchor_texts = anchor_texts\n",
    "        self.positive_texts = positive_texts\n",
    "        self.negative_texts = negative_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anchor_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_text = self.anchor_texts[idx]\n",
    "        positive_text = self.positive_texts[idx]\n",
    "        negative_text = self.negative_texts[idx]\n",
    "\n",
    "        anchor = self.tokenizer(anchor_text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        positive = self.tokenizer(positive_text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        negative = self.tokenizer(negative_text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        return anchor, positive, negative\n",
    "    \n",
    "\n",
    "# Pretrained BERT model for embeddings\n",
    "class BertEmbeddingNet(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
    "        super(BertEmbeddingNet, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the CLS token's output as the sentence embedding\n",
    "        return output.last_hidden_state[:, 0, :]  # [batch_size, hidden_dim]\n",
    "\n",
    "\n",
    "# Triplet loss function\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.loss_fn = nn.TripletMarginLoss(margin=margin)\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        return self.loss_fn(anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (example placeholders)\n",
    "anchor_texts = [\"king\", \"queen\", \"paris\"]\n",
    "positive_texts = [\"monarch\", \"monarch\", \"france\"]\n",
    "negative_texts = [\"dog\", \"cat\", \"germany\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TripletTextDataset(anchor_texts, positive_texts, negative_texts, tokenizer, max_len=8)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[ 101, 2332,  102,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101, 11590,   102,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 3899,  102,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0]])})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network, loss function, and optimizer\n",
    "embedding_net = BertEmbeddingNet(pretrained_model_name='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss_fn = TripletLoss(margin=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(embedding_net.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 3.3852\n",
      "Epoch [2/5], Loss: 0.3529\n",
      "Epoch [3/5], Loss: 2.0858\n",
      "Epoch [4/5], Loss: 0.9439\n",
      "Epoch [5/5], Loss: 0.8659\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    embedding_net.train()\n",
    "    running_loss = 0.0\n",
    "    for anchors, positives, negatives in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Process anchors\n",
    "        anchor_input_ids = anchors['input_ids'].squeeze(1)  # Remove extra batch dimension\n",
    "        anchor_attention_mask = anchors['attention_mask'].squeeze(1)\n",
    "        anchor_embeddings = embedding_net(anchor_input_ids, anchor_attention_mask)\n",
    "\n",
    "        # Process positives\n",
    "        positive_input_ids = positives['input_ids'].squeeze(1)\n",
    "        positive_attention_mask = positives['attention_mask'].squeeze(1)\n",
    "        positive_embeddings = embedding_net(positive_input_ids, positive_attention_mask)\n",
    "\n",
    "        # Process negatives\n",
    "        negative_input_ids = negatives['input_ids'].squeeze(1)\n",
    "        negative_attention_mask = negatives['attention_mask'].squeeze(1)\n",
    "        negative_embeddings = embedding_net(negative_input_ids, negative_attention_mask)\n",
    "\n",
    "        # Compute the triplet loss\n",
    "        loss = triplet_loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
