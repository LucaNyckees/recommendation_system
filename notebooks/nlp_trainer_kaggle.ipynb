{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucanyckees/Desktop/my-repos/recommendation_system/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/lucanyckees/Desktop/my-repos/recommendation_system/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/lucanyckees/Desktop/my-repos/recommendation_system/venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments,Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Set use_fast=False to avoid using the fast tokenizer\n",
    "model_nm = 'microsoft/deberta-v3-small'\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/nlp_trainer_dataset.csv\")\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/884 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 884/884 [00:00<00:00, 27384.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tok_func(x): return tokz(x[\"input\"])\n",
    "tok_ds = ds.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = tok_ds.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 36/36 [00:00<00:00, 9597.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_df = pd.read_csv(\"../data/nlp_test_kaggle.csv\")\n",
    "eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dds = tok_ds.train_test_split(0.25, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "epochs = 4\n",
    "lr = 8e-5\n",
    "\n",
    "def corr(x,y): return np.corrcoef(x,y)[0][1]\n",
    "def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='outputs',\n",
    "    learning_rate=lr,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    fp16=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucanyckees/Desktop/my-repos/recommendation_system/notebooks/nlp_trainer_kaggle.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucanyckees/Desktop/my-repos/recommendation_system/notebooks/nlp_trainer_kaggle.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucanyckees/Desktop/my-repos/recommendation_system/notebooks/nlp_trainer_kaggle.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucanyckees/Desktop/my-repos/recommendation_system/notebooks/nlp_trainer_kaggle.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucanyckees/Desktop/my-repos/recommendation_system/notebooks/nlp_trainer_kaggle.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mdds[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucanyckees/Desktop/my-repos/recommendation_system/notebooks/nlp_trainer_kaggle.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mdds[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucanyckees/Desktop/my-repos/recommendation_system/notebooks/nlp_trainer_kaggle.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokz,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucanyckees/Desktop/my-repos/recommendation_system/notebooks/nlp_trainer_kaggle.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcorr_d,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucanyckees/Desktop/my-repos/recommendation_system/notebooks/nlp_trainer_kaggle.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/my-repos/recommendation_system/venv/lib/python3.11/site-packages/transformers/trainer.py:411\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_in_train \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_accelerator_and_postprocess()\n\u001b[1;32m    413\u001b[0m \u001b[39m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_memory_tracker \u001b[39m=\u001b[39m TrainerMemoryTracker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m~/Desktop/my-repos/recommendation_system/venv/lib/python3.11/site-packages/transformers/trainer.py:4858\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4855\u001b[0m     args\u001b[39m.\u001b[39mupdate(accelerator_config)\n\u001b[1;32m   4857\u001b[0m \u001b[39m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 4858\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator \u001b[39m=\u001b[39m Accelerator(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   4859\u001b[0m \u001b[39m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   4860\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mgather_for_metrics\n",
      "File \u001b[0;32m~/Desktop/my-repos/recommendation_system/venv/lib/python3.11/site-packages/accelerate/accelerator.py:477\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, dispatch_batches, even_batches, use_seedable_sampler, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnative_amp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mxpu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnpu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmlu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmusa\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    475\u001b[0m     check_is_tpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    476\u001b[0m ):\n\u001b[0;32m--> 477\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype\u001b[39m!r}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    478\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler_handler\u001b[39m.\u001b[39mto_kwargs() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_type \u001b[39m==\u001b[39m DistributedType\u001b[39m.\u001b[39mFSDP:\n",
      "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dds['train'],\n",
    "    eval_dataset=dds['test'],\n",
    "    tokenizer=tokz,\n",
    "    compute_metrics=corr_d,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(eval_ds).predictions.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
